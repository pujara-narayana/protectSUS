"""Vulnerability Assessment Agent (VAA)"""

from typing import Dict, Any
import logging

from app.services.agents.base_agent import BaseAgent
from app.models.analysis import Vulnerability

logger = logging.getLogger(__name__)


class VulnerabilityAgent(BaseAgent):
    """Agent for detecting security vulnerabilities in code"""

    def __init__(self):
        super().__init__("VulnerabilityAgent")

    async def analyze(self, code: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze code for security vulnerabilities

        Detects: SQL injection, XSS, CSRF, authentication issues,
        authorization flaws, sensitive data exposure, etc.
        """
        logger.info(f"{self.name}: Starting vulnerability analysis")

        system_prompt = """You are an expert security researcher specializing in application security and vulnerability assessment.

Your task is to analyze the provided code for security vulnerabilities following OWASP Top 10 and CWE guidelines.

Focus on detecting:
1. Injection flaws (SQL, NoSQL, Command, LDAP, XPath)
2. Cross-Site Scripting (XSS) - reflected, stored, DOM-based
3. Cross-Site Request Forgery (CSRF)
4. Broken Authentication and Session Management
5. Security Misconfiguration
6. Sensitive Data Exposure
7. Missing Access Control
8. XML External Entities (XXE)
9. Insecure Deserialization
10. Using Components with Known Vulnerabilities

For each vulnerability found, provide output in this EXACT format:

FILE: [file path]
LINE: [line number]
SEVERITY: [critical|high|medium|low]
TYPE: [vulnerability type, e.g., SQL_INJECTION, XSS]
CWE: [CWE-XXX]
DESCRIPTION: [detailed description of the vulnerability]
FIX: [recommended fix]

Be thorough but avoid false positives. Only report actual security issues, not code quality concerns."""

        user_prompt = f"""Analyze this codebase for security vulnerabilities:

{code}

Repository context:
- Files analyzed: {context.get('file_mapping', {}).get('total_files', 'unknown')}
- Compressed: {context.get('compression_ratio', 0):.1%}

Provide detailed findings following the specified format."""

        try:
            # Call LLM API
            response = await self._call_llm(system_prompt, user_prompt)

            # Extract structured findings
            findings = self._extract_findings(response, 'vulnerability')

            # Convert to Vulnerability models
            vulnerabilities = []
            for finding in findings:
                if all(k in finding for k in ['file_path', 'line_number', 'severity', 'type']):
                    try:
                        vuln = Vulnerability(
                            type=finding['type'],
                            severity=finding['severity'],
                            file_path=finding['file_path'],
                            line_number=finding['line_number'],
                            description=finding.get('description', ''),
                            cwe_id=finding.get('cwe_id'),
                            recommended_fix=finding.get('recommended_fix')
                        )
                        vulnerabilities.append(vuln)
                    except Exception as e:
                        logger.warning(f"Failed to create Vulnerability from finding: {e}")

            logger.info(f"{self.name}: Found {len(vulnerabilities)} vulnerabilities")

            return {
                'agent_name': self.name,
                'vulnerabilities': [v.model_dump() for v in vulnerabilities],
                'raw_response': response,
                'findings_count': len(vulnerabilities)
            }

        except Exception as e:
            logger.error(f"{self.name}: Analysis failed: {e}")
            raise
